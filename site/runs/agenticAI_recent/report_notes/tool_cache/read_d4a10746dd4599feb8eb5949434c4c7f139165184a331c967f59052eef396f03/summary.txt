- **제목**: V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval
- **저자**: Dongyang Chen 외 8명
- **홈페이지**: [GitHub](https://github.com/chendy25/V-Retrver), [Hugging Face](https://huggingface.co/V-Retrver)
- **요약**:
  - MLLMs가 다중 모달 검색에 적용되며, Chain-of-Thought (CoT) 추론이 후보 재정렬을 개선함.
  - 기존 접근법은 언어 중심으로, 정적 시각 인코딩에 의존하여 세밀한 시각 증거 검증 능력이 부족함.
  - V-Retrver는 시각 검증 기반의 에비던스 중심 검색 프레임워크를 제안함.
  - MLLM이 외부 시각 도구를 통해 시각 증거를 선택적으로 획득하여 가설 생성과 목표 시각 검증을 번갈아 수행함.
  - 커리큘럼 기반 학습 전략을 채택하여 여러 다중 모달 검색 벤치마크에서 평균 23.0%의 검색 정확도 향상을 보여줌.
- **소개**:
  - MLLMs의 발전이 다중 모달 검색을 크게 향상시킴.
  - CoT 추론 통합으로 검색 성능과 해석 가능성을 개선할 수 있음.
  - 기존 CoT 기반 검색 시스템은 언어 중심으로 시각 증거 의존 결정에서 한계가 있음.
- **문제점**:
  - 현재 MLLM 기반 검색 방법들은 시각적 입력을 고정된 임베딩이나 텍스트 설명으로 압축하여 언어에 의존하게 만듦.
  - 이로 인해 모델은 시각적 차이를 추론할 때 종종 허위의 추론을 생성함.
  - 최근 검색 프레임워크인 Retrv-R1과 MM-R5는 텍스트 추론의 깊이를 개선했지만, 단일 패스 시각 인코딩에 의존하여 시각적 가설을 검증할 수 없음.
- **V-Retrver의 기능**:
  - 시각적 검사를 기반으로 한 에비던스 중심 검색 프레임워크.
  - MLLM이 추론 중에 시각적 증거를 선택적으로 획득할 수 있도록 외부 시각 도구를 호출함.
  - 다중 모달 교차 사고 과정을 통해 모델은 가설 생성과 목표 시각 검증을 번갈아 수행하여 시각적 모호성을 동적으로 해결하고 순위를 점진적으로 개선함.

출처: [chunk_001.txt], [chunk_002.txt]