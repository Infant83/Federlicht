{"source": "recent_search", "query": "Agentic AI security reliability risks", "paper": {"arxiv_id": "2602.06043v1", "title": "Shared LoRA Subspaces for almost Strict Continual Learning", "authors": ["Prakhar Kaushik", "Ankit Vaidya", "Shravan Chaudhari", "Rama Chellappa", "Alan Yuille"], "published": "2026-02-05T18:59:58+00:00", "updated": "2026-02-05T18:59:58+00:00", "summary": "Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CV"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2602.06043v1", "entry_id": "http://arxiv.org/abs/2602.06043v1", "cited_by_count": 0}}
{"source": "recent_search", "query": "Agentic AI security reliability risks", "paper": {"arxiv_id": "2602.06039v1", "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching", "authors": ["Yuxing Lu", "Yucheng Hu", "Xukai Zhao", "Jiuxin Cao"], "published": "2026-02-05T18:59:51+00:00", "updated": "2026-02-05T18:59:51+00:00", "summary": "Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \\key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.", "primary_category": "cs.AI", "categories": ["cs.AI"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2602.06039v1", "entry_id": "http://arxiv.org/abs/2602.06039v1", "cited_by_count": 0}}
{"source": "recent_search", "query": "Agentic AI security reliability risks", "paper": {"arxiv_id": "2602.06038v1", "title": "CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction", "authors": ["Xiaopan Zhang", "Zejin Wang", "Zhixu Li", "Jianpeng Yao", "Jiachen Li"], "published": "2026-02-05T18:59:45+00:00", "updated": "2026-02-05T18:59:45+00:00", "summary": "To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2602.06038v1", "entry_id": "http://arxiv.org/abs/2602.06038v1", "cited_by_count": 0}}
{"source": "recent_search", "query": "Agentic AI security reliability risks", "paper": {"arxiv_id": "2602.06035v1", "title": "InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions", "authors": ["Sirui Xu", "Samuel Schulter", "Morteza Ziyadi", "Xialin He", "Xiaohan Fei", "Yu-Xiong Wang", "Liangyan Gui"], "published": "2026-02-05T18:59:27+00:00", "updated": "2026-02-05T18:59:27+00:00", "summary": "Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.", "primary_category": "cs.CV", "categories": ["cs.CV", "cs.GR", "cs.RO"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2602.06035v1", "entry_id": "http://arxiv.org/abs/2602.06035v1", "cited_by_count": 0}}
{"source": "recent_search", "query": "Agentic AI security reliability risks", "paper": {"arxiv_id": "2602.06034v1", "title": "V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval", "authors": ["Dongyang Chen", "Chaoyang Wang", "Dezhao SU", "Xi Xiao", "Zeyu Zhang", "Jing Xiong", "Qing Li", "Yuzhang Shang", "Shichao Ka"], "published": "2026-02-05T18:59:21+00:00", "updated": "2026-02-05T18:59:21+00:00", "summary": "Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.", "primary_category": "cs.CV", "categories": ["cs.CV"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2602.06034v1", "entry_id": "http://arxiv.org/abs/2602.06034v1", "cited_by_count": 0}}
{"source": "recent_search", "query": "Agentic AI governance multi-agent interaction responsibility", "paper": {"arxiv_id": "2602.06041v1", "title": "Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning", "authors": ["Xuejun Zhang", "Aditi Tiwari", "Zhenhailong Wang", "Heng Ji"], "published": "2026-02-05T18:59:55+00:00", "updated": "2026-02-05T18:59:55+00:00", "summary": "Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20Â° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.", "primary_category": "cs.CV", "categories": ["cs.CV"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2602.06041v1", "entry_id": "http://arxiv.org/abs/2602.06041v1", "cited_by_count": 0}}
{"source": "recent_search", "query": "Agentic AI cost delay evaluation challenges", "paper": {"arxiv_id": "2602.06040v1", "title": "SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs", "authors": ["Jintao Tong", "Shilin Yan", "Hongwei Xue", "Xiaojun Tang", "Kunyu Shi", "Guannan Zhang", "Ruixuan Li", "Yixiong Zou"], "published": "2026-02-05T18:59:51+00:00", "updated": "2026-02-05T18:59:51+00:00", "summary": "Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as \"visual thoughts\" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.", "primary_category": "cs.CV", "categories": ["cs.CV"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2602.06040v1", "entry_id": "http://arxiv.org/abs/2602.06040v1", "cited_by_count": 0}}
