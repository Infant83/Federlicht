

===== PAGE 1 =====
SwimBird: Eliciting Switchable Reasoning Mode in
Hybrid Autoregressive MLLMs
Jintao Tong1,2
Shilin Yan2†‡
Hongwei Xue2
Xiaojun Tang2
Kunyu Shi2
Guannan Zhang2
Ruixuan Li1‡
Yixiong Zou1‡
1Huazhong University of Science and Technology
2Accio Team, Alibaba Group
† Project Leader
‡ Corresponding Author
Abstract
Multimodal Large Language Models (MLLMs) have made remarkable progress in
multimodal perception and reasoning by bridging vision and language. However, most existing
MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on
vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as “visual
thoughts” into the reasoning process and improve visual performance, but often at the cost of degraded
text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning
pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We
introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning
modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden
states as visual thoughts), and (3) interleaved vision–text reasoning. To enable this capability, we
adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with
next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation
strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three
reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong
textual logic while substantially improving performance on vision-dense tasks. Experiments across
diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate
that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal
reasoning methods.
Project Page: https://accio-lab.github.io/SwimBird
Github Repo: https://github.com/Accio-Lab/SwimBird
HuggingFace: https://huggingface.co/datasets/Accio-Lab/SwimBird-SFT-92K
1
Introduction
Building on the success of Chain-of-Thought (CoT) [32, 9] reasoning in LLMs, recent multimodal
research has adopted step-by-step reasoning to decompose complex vision-and-language problems
into intermediate steps that are easier to solve. With textual CoT, Multimodal Large Language Models
(MLLMs) [44, 7, 16, 24] have significantly improved on tasks requiring symbolic manipulation,
numerical calculation, and logical analysis.
However, this success does not fully transfer to vision-dense tasks where the bottleneck lies in dense
perception and spatial reasoning rather than logical structure [4]. Typical examples include maze
solving, fine-grained visual search, and other problems where accurate intermediate visual states
are essential. On such tasks, purely textual CoT [19] can be an ill-posed interface: the model is
forced to describe intermediate visual evidence in language even when language is not a faithful
carrier, causing brittle reasoning and error accumulation [37]. To address this, recent works introduce
latent visual reasoning [11, 22] that supervises models to generate semantically grounded continuous
arXiv:2602.06040v1  [cs.CV]  5 Feb 2026


===== PAGE 2 =====
<reason>The rectangle has a width of 1 yard and a height of 2 
yards. The perimeter of a rectangle is calculated using the formula:
Perimeter = 2 × (width + height) 
Substituting the values: 
Perimeter = 2 × (1 + 2) = 2 × 3 = 6 yards.
Thus, the perimeter is 6 yards.</reason> 
<answer>6</answer> 
<think>…… To accurately determine the 
dimensions, it's helpful to visualize the 
rectangle with its sides clearly 
labeled. ……</think>
<latent> latent embedding </latent> 
<think> I can clearly see that the length 
of the rectangle is 1 yard and the width 
is also 1 yard. Now, I will substitute these 
values into the perimeter formula: P = 
2(1 yd + 1 yd) …….</think>
<answer>4</answer>
<latent> latent embedding </latent>
<answer>4</answer>
Question: What is the 
perimeter of the rectangle?
1 yd
1 yd
2 yd
2 yd
Correct Answer: 6
The rectangle has:
- Two sides of length 1 yard
- Two sides of length 2 yards
The perimeter is the total distance 
around the shape.
Perimeter = 1 yd + 2 yd + 1 yd + 2 yd
Perimeter = (1 + 2 + 1 + 2) yd
Perimeter = 6 yd
The perimeter of the rectangle is 6 yards.
Textual CoT Method
Unified CoT Method
Modality Redundant
Latent Visual CoT Method
❌
Modality Mismatch
SwimBird (Switchable Reasoning Mode)
What object is situated in 
the corner of the room 
depicted in this image?
Answer: Television
❌
Unified CoT Method
<think>…… To clearly pinpoint the 
object, I will highlight the relevant 
area with a bounding box.</think>
<latent> latent embedding </latent> 
<think>The red bounding box isolates 
a small, rectangular electronic device 
on a wooden stand, which appears to 
be an old television set.…….</think>
<answer>tv</answer>
<reason> To answer the question, I need to identify the object 
located in the corner of the provided image. I will mark it with a 
bounding box for clarity.</reason>
<|latent_start|> latent embedding <|latent_end|> 
<reason> The red box highlights a small rectangular electronic 
device on a wooden stand that appears to be an old-style 
television; Based on the highlighted details, it can be identified 
as a television. </reason>
<answer>Television.</answer>
SwimBird (Switchable Reasoning Mode)
<latent> latent embedding </latent>
<answer>Chair</answer>
Latent Visual CoT Method
❌
Based on the image provided, the object 
situated in the corner of the room is an 
**air conditioner**.
Textual CoT Method
❌
Quantify the relocation steps 
between green and red positions
Correct Answer: 4
… …
The path is as follows:
- From the green square, move right 
to the red square.
There is only one step involved in this 
movement.
Therefore, the number of relocation 
steps between the green and red 
positions is **1**
Textual CoT Method
<think>…... it's essential to visually 
trace the most efficient path through 
the maze, ensuring that all black wall 
segments are avoided. …….</think>
<latent> latent embedding </latent> 
<think>…... Starting from the green 
square and following the blue line 
until reaching the red square, I find 
that the total number of steps 
required is 20.</think>
<answer>It takes 20 steps.</answer>
Modality Redundant
Unified CoT Method
<|latent_start|> latent embedding <|latent_end>
<answer>4</answer> 
<latent> latent embedding </latent>
<answer>4</answer>
Latent Visual CoT Method
Modality Mismatch
❌
❌
SwimBird (Switchable Reasoning Mode)
Figure 1: SwimBird enables query-adaptive multimodal reasoning by dynamically switching
among text-only, vision-only, and interleaved vision–text modes. As illustrated, it avoids redundant
latent steps on text-centric queries (Case 1), relies on latent visual thoughts for vision-dense spatial
problems (Case 2), and interleaves visual grounding with textual deduction when both are needed
(Case 3), mitigating modality mismatch and improving robustness.
hidden states as visual thoughts, enabling intermediate visual representations to be maintained and
updated across steps, which substantially strengthens performance on vision-dense benchmarks.
Despite these advances, existing multimodal CoT designs largely rely on a rigid, pre-defined reasoning
pattern. Concretely, prior methods [25, 39, 36] typically fall into three fixed paradigms: text-only
CoT, vision-only CoT, or interleaved vision–text CoT. As shown in Fig. 1, such fixed patterns create a
mismatch between the reasoning modality and the actual needs of the question: forcing visual thoughts
for text-centric queries can interfere with discrete symbolic reasoning, while restricting strongly
visual queries to text-only reasoning removes an appropriate latent workspace. Even interleaved
reasoning remains a fixed schedule that may generate redundant modality steps [23].
We argue that the core limitation is the assumption that a single, static reasoning template can
generalize across heterogeneous multimodal queries. Different questions demand different internal
computation formats. Some require only discrete symbolic steps, some require only latent visual
transitions, and some require tight alternation between visual grounding and textual deduction. A
more capable MLLM should therefore be able to choose when to think in language, when to think
in vision, conditioned on the input and the evolving reasoning state.
Motivated by this, we propose SwimBird, a reasoning-switchable MLLM for query-adaptive mul-
timodal reasoning. SwimBird is built on two key ideas derived from the limitations above. First,
we adopt a hybrid autoregressive formulation that supports both (i) standard next-token prediction
for textual thoughts and (ii) next-embedding prediction for continuous visual thoughts. This unified
generation interface provides the foundation for switchable reasoning. Second, we attribute the rigid-
ity of prior patterns partly to training data bias. We therefore design a systematic curation strategy
that filters and categorizes multimodal CoT samples into reasoning modes based on their visual
dependency and reasoning characteristics. Through this strategy, we construct SwimBird-SFT-92K,
a diverse supervised fine-tuning dataset covering text-only, vision-only, and interleaved vision–text
patterns. With these designs, SwimBird can dynamically switch among three reasoning modes.
2


===== PAGE 3 =====
Importantly, SwimBird also removes the fixed-budget constraint in visual reasoning. Instead of
generating a constant-length sequence of visual thought tokens, it dynamically determines the number
of visual thought tokens during vision-only or interleaved reasoning, allocating more latent compu-
tation to vision-dense queries while avoiding redundant visual thoughts for text-centric problems.
As a result, a single model can robustly handle diverse query types, whereas fixed-pattern baselines
typically excel only on a subset and may underperform when the required thinking modality or
visual-thought budget deviates from their pre-defined design.
Our contributions are summarized as follows:
• We identify two key bottlenecks of prior multimodal CoT frameworks, namely fixed reasoning-
mode templates and fixed visual-thought lengths, and show how they lead to a modality mismatch
that harms either vision-dense performance or text-based logical reasoning.
• We introduce SwimBird, a hybrid autoregressive MLLM that can dynamically switch among text-
only, vision-only, and interleaved reasoning modes, combining next-token prediction for textual
thoughts with next-embedding prediction for visual thoughts.
• We further introduce adaptive visual-thought allocation, enabling SwimBird to dynamically deter-
mine the number of continuous visual-thought tokens based on query complexity.
• We design a systematic reasoning-mode curation strategy for multimodal CoT samples and construct
SwimBird-SFT-92K, a dataset covering three reasoning patterns that enables query-adaptive mode
selection.
• Extensive experiments across diverse benchmarks demonstrate that SwimBird achieves state-of-the-
art performance on both text-centric reasoning and challenging vision-dense tasks, outperforming
prior fixed-pattern multimodal reasoning methods.
2
Related Works
2.1
Textual CoT in MLLMs
The integration of vision and language has evolved from discriminative tasks toward generative
reasoning frameworks. Early MLLMs focus primarily on visual question answering through direct
answer generation [13, 15, 27, 14, 35]. With the success of step-by-step reasoning in LLMs, recent
MLLMs incorporate explicit reasoning chains to handle complex multimodal problems [1, 29, 34].
These models generate intermediate textual explanations before producing final answers, demonstrat-
ing improved performance on mathematical word problems, scientific diagram understanding, and
multi-hop visual reasoning [38, 31, 17]. Despite their effectiveness on logic-heavy benchmarks, these
text-based reasoning approaches struggle when the core challenge lies in visual perception rather than
logical decomposition [20]. Tasks requiring spatial transformation tracking, visual state prediction, or
fine-grained visual comparison expose the fundamental limitation that the model is forced to describe
intermediate visual evidence in language, even when language is not a faithful or efficient carrier for
the required information, leading to brittle reasoning and error accumulation.
2.2
Latent Visual Reasoning
Recognizing the constraints of language-only reasoning, researchers have explored alternative com-
putational substrates for visual thinking [18, 28]. Recent methods propose latent visual reasoning
by training models to produce continuous embeddings supervised by visual reconstruction objec-
tives. For instance, Mirage [36] employs hidden states trained to approximate annotated helper
images, while LVR [11] focuses on reconstructing cropped image regions. SkiLa [22] proposes
unified reasoning that alternates between generating latent visual tokens and discrete textual tokens.
However, existing latent reasoning methods uniformly apply the same reasoning structure across all
inputs: models trained with visual thoughts always generate them, even for purely textual queries.
Furthermore, these methods use fixed-length latent tokens regardless of whether a problem requires
minimal or extensive visual deliberation. SwimBird addresses both limitations through dynamic mode
selection and adaptive visual token budgets, enabling truly query-adaptive multimodal reasoning.
3


===== PAGE 4 =====
For Textual Thought
Next Token Prediction
SwimBird
…
LLM Head
SwimBird
<reason>The image … </reason>
Last Hidden States
…
Shifted CE Loss
Target Logits
Predicted Logits
For Visual Thought
Next Embedding Prediction
Shifted 
MSE Loss
Target Embedding
Predicted Embedding
Inference
…
<latent>
</latent>
Answer
Embedding
…
Textual CoT
…
<latent>
</latent>
Answer
Embedding
…
Vision Only (Dynamic Latent Visual Token Num)
Interleave Vision-Text
<latent>
</latent>
…
Answer
…
Textual CoT
Text Only
Figure 2: SwimBird adopts a hybrid autoregressive formulation that performs next-token prediction
for textual thoughts and switches to next-embedding prediction for visual thoughts. During
inference, SwimBird performs query-adaptive multimodal reasoning by dynamically selecting among
three modes conditioned on the input: text-only, vision-only, and interleaved vision-text reasoning.
3
Method
SwimBird adopts a hybrid autoregressive formulation that supports both discrete textual tokens
and continuous latent visual tokens. As shown in Fig. 2 (left), it performs standard next-token
prediction for textual thoughts, optimized with a shifted cross-entropy loss, and performs next-
embedding prediction for visual thoughts, optimized with a MSE loss to reconstruct the embeddings
of intermediate thinking images. During inference (Fig. 2 right), SwimBird performs query-adaptive
reasoning by generating either (i) text-only traces, (ii) vision-only traces with a variable-length latent
span, or (iii) interleaved vision–text traces, conditioned on the input.
3.1
Hybrid Autoregressive Modeling
Textual thought as next-token prediction. For textual reasoning spans, SwimBird behaves like a
standard language model. Given a token sequence {w1, . . . , wT }, the model outputs logits parame-
terizing
pθ(wt | w<t, x),
(1)
where x denotes the observed image (and prior context). We train these spans with the standard
cross-entropy loss:
Ltext = −
T
X
t=1
log pθ(wt | w<t, x).
(2)
This objective preserves the discrete symbolic manipulation and logical consistency of the language
backbone, which is essential for text-centric reasoning tasks.
Visual thought as next-embedding prediction. For vision-only reasoning or visual segments inside
interleaved reasoning, SwimBird generates a sequence of continuous latent tokens (visual thoughts)
{z1, . . . , zK}, each represented as a hidden-state embedding rather than a discrete word. Concretely,
we treat each visual-thought step as predicting the next embedding in an autoregressive manner:
ˆzk = fθ(z<k, w≤T , x),
(3)
and supervise it with a shifted mean squared error (MSE) loss against target embeddings zk:
Lvis =
K
X
k=1
∥ˆzk −zk∥2
2 .
(4)
Here, the target embeddings are computed by encoding the intermediate thinking images with the
same vision encoder (and projection) used by SwimBird, thus grounding latent visual thoughts in
semantically meaningful visual states.
Unified training objective. A training instance may contain pure textual CoT, pure visual CoT, or
interleaved segments. We optimize a unified objective that sums modality-specific losses over the
activated segments:
L = λtextLtext + λvisLvis,
(5)
4


===== PAGE 5 =====
Data Source
All Mode
Text Only
Vision Only
Interleave
Problem Domain
Zebra-CoT
26.3K
0
5.9K
20.4K
Visual Search, Jigsaw, Maze, Geometry, Chess...
ThinkMorph
7.1K
0
1.2K
5.9K
Visual Search, Spatial Navigation, Jigsaw, Chart
MathCanvas
8.9K
0
1.7K
7.2K
Geometry, Algebra, Calculus, Statistics
OpenMMReasoner
50K
50K
0
0
General VQA, Math VQA, Text QA
Total
92.3K
50K
8.8K
33.5K
Table 1: Detailed statistics of SwimBird-SFT-92K.
where λtext and λvis are balancing coefficients. In practice, each sample only contributes to the losses
of the modes it contains, enabling the model to learn all three reasoning patterns without forcing
unnecessary supervision.
Mode switching with special delimiters To enable controllable and learnable switching among
reasoning modes, we introduce explicit delimiters in the target sequences. Specifically, we mark
visual-thought spans using special tokens such as <|latent_start|> and <|latent_end|>. Dur-
ing training, these delimiters define where the model should produce continuous latent embeddings
instead of textual tokens. During inference, SwimBird generates these delimiters autoregressively,
which makes mode selection query-adaptive: the model can decide whether to enter a latent visual-
thinking phase, remain in text-only reasoning, or alternate between the two (Fig. 2 right).
3.2
Dynamic Latent Token Budget
Vision Encoder
Fixed latent Tokens
Pooling
Vision Encoder
Intermediate Thinking Images Have Different Resolutions
Independent
[min pixels, max pixels]
Dynamic Latent Tokens
Other Methods
SwimBird
Figure 3: Resolution-aware, dynamic latent to-
kens budget.
Prior latent visual reasoning methods typically
adopt a fixed number of latent tokens (or a fixed
pooling strategy) for all inputs. This design has
two drawbacks: (1) it can lead to insufficient ca-
pacity for vision-dense, high-resolution images,
while wasting computation on vision-easy, low-
resolution images; (2) pooling intermediate pro-
cess images into a fixed token length during train-
ing may discard spatial details, making it harder
for the model to learn semantically meaningful
latent embeddings.
As shown in Figure 3, SwimBird addresses these
issues with a resolution-aware, dynamic latent to-
ken budget. Benefiting from the naive-resolution
property of the Qwen ViT, we assign different
maximum pixel budgets to the question image
and the intermediate thinking images during training, which directly controls the maximum number
of visual tokens produced by the vision encoder for each type of image. Concretely, we allow the
vision encoder to output a variable number of visual tokens according to image resolution, bounded
by an independent range [Nmin, Nmax] (implemented via pixel/patch budget control). This avoids
aggressive pooling that discards fine-grained evidence, while preventing excessively long visual
sequences from dominating computation. Consequently, SwimBird can preserve detailed visual
information when needed (e.g., tiny targets or dense diagrams) and remain efficient on simpler cases.
With this resolution-aware training setup, SwimBird further learns to allocate latent computation
dynamically at inference time. In vision-only and interleaved modes, the number of latent tokens K
is not pre-defined: the model keeps generating latent embeddings until it decides to stop by emitting
</latent>. This variable-length latent span naturally matches the amount of visual thinking to the
perceived difficulty of the query.
3.3
Switchable Reasoning SFT Dataset Construction
To enable switchable reasoning modes, we curate a diverse SFT dataset covering three reasoning
patterns: (1) text-only CoT, (2) vision-only CoT where intermediate images are sufficient, and (3)
interleaved vision-text CoT requiring both modalities. Our curation pipeline consists of three stages:
5


===== PAGE 6 =====
Model
V* Bench
HR-Bench 4K
HR-Bench 8K
MME RealWorld
Avg.
Textual Reasoning Models
GPT-4o [8]
66.0
59.0
55.5
62.8
60.9
GPT-5-mini
63.9
66.3
60.9
-
-
Qwen2.5-VL-32B-Instruct
80.6
69.3
63.6
59.1
68.2
Qwen2.5-VL-7B-Instruct
75.3
65.5
62.1
56.8
64.9
Qwen3-VL-8B-Instruct *
83.8
76.5
71.3
61.9
73.4
Qwen3-VL-8B-Thinking
77.5
72.4
68.1
-
-
InternVL3-8B [46]
81.2
70.0
69.3
-
-
LLaVA-OneVison [12]
75.4
63.0
59.8
57.4
63.9
Vision-R1 [7]
80.1
64.8
57.0
-
-
Latent Visual Reasoning Models
Monet [28]
83.3
71.0
68.0
-
-
LVR [11]
81.7
69.6
66.1
-
-
SkiLa [22]
84.3
72.0
66.5
-
-
Multimodal Agentic Models
SEAL [33]
74.8
-
-
-
-
Pixel Reasoner [26]
84.3
72.6
66.1
64.4
71.9
DeepEyes [45]
83.3
73.2
69.5
64.1
72.5
Thyme [42]
82.2
77.0
72.0
64.8
74.0
DeepEyesV2 [6]
81.8
77.9
73.8
64.9
74.6
SwimBird
85.5
79.0
74.9
65.3
76.2
Table 2: Performance on fine-grained visual understanding benchmarks. Here, * denotes the results
are reproduced by ourselves.
Stage 1: Candidate collection and easy-instance filtering. We collect raw image-text interleaved
CoT data from ThinkMorph [5], Zebra-CoT [10], and MathCanvas-Instruct [21]. These datasets
provide multimodal reasoning chains with intermediate visual thinking steps. where each sample
contains intermediate thinking images. To focus on cases where intermediate visual reasoning is
useful, we remove instances that are already solvable from the original input: Qwen3VL-8B is
evaluated on the question and the original image, and correctly answered samples are filtered out.
Stage 2: Reasoning-mode labeling via pass@8. For each remaining sample, we compute two
pass@8 scores with Qwen3VL-8B: passbase using only the question and problem image, and passhint
additionally providing the intermediate thinking images as visual hints. We judge each sampled
answer using Qwen3-235B-Instruct given the question, prediction, and ground truth. We keep
samples with passhint ≥passbase, indicating that intermediate thinking images provide non-negative
gains. Among them, we label samples with passhint ≥0.75 as vision-only, since the model can
solve the problem with high probability using the intermediate thinking images without an explicit
textual CoT. The remaining kept samples, where passhint ≥passbase but passhint < 0.75, are labeled
as interleaved vision–text, since the images help but are insufficient for consistently correct solutions
and textual reasoning is still needed. This procedure yields 42K high-quality SFT samples covering
the vision-only and interleaved modes.
Stage 3: Add text-only CoT data. To complete the three-mode training set, we sample 50K text-only
CoT instances from OpenMMReasoner [40], which provides pass@8-filtered textual CoT traces.
Combining them with the 42K samples from Stage 2 yields SwimBird-SFT-92K, covering text-only,
vision-only, and interleaved vision–text patterns. Detailed statistics are reported in Table 1.
4
Experiments
Training Details We adopt Qwen3-VL 8B [1] as the base model and conduct supervised fine-tuning
on our curated SwimBird-SFT-92K. Training is performed on A100-80G GPUs with a global batch
size of 128. The vision encoder and multimodal projector are kept frozen, and only the LLM
parameters are updated. A cosine learning rate scheduler is applied with an initial learning rate of
1e-5.
6


===== PAGE 7 =====
Models
General VQA
Multimodal Reasoning
MMStar
RealWorldQA
WeMath
DynaMath
MathVerse_MINI
Qwen2.5-VL-32B-Instruct
70.3
-
-
-
48.5
Qwen2.5-VL-7B-Instruct
60.3
67.4
34.6
53.3
45.6
Qwen3-VL-8B-Instruct *
64.7
71.8
38.8
65.3
61.3
LLaVA-OneVision [12]
61.9
69.9
20.9
-
19.3
DeepEyes [45]
-
-
38.9
55.0
47.3
DeepEyesV2 [6]
-
-
38.1
57.2
52.7
SkiLa [22]
64.8
69.3
-
-
-
SwimBird
71.2
73.1
49.5
67.2
65.8
Table 3: Performance on general vqa and multimodal reasoning tasks. Here, * denotes the results
are reproduced by ourselves.
Baselines and Benchmarks To comprehensively assess the effectiveness of SwimBird, we compare it
against three categories of baselines: (1) textual reasoning models, including advanced closed-source
systems (e.g., GPT-4o and GPT-5-mini) and state-of-the-art open-source models (e.g., Qwen2.5/3-VL,
LLaVA-OneVision); (2) latent visual reasoning models (e.g., Monet, LVR, SkiLa); and (3) multi-
modal agentic models that rely on explicit tool/workflow designs (e.g., Pixel Reasoner, DeepEyes,
Thyme). We evaluate on two groups of benchmarks: (i) fine-grained/high-resolution visual under-
standing (V* Bench [33], HR-Bench 4K/8K [30], MME-RealWorld [43]; Table 2), and (ii) general
VQA and multimodal reasoning (MMStar [2], RealWorldQA [3], WeMath [17], DynaMath [47],
MathVerse_MINI [41]; Table 3). Results marked with * are reproduced by ourselves.
4.1
Main Results
Fine-grained Visual Understanding Table 2 demonstrates that SwimBird achieves state-of-the-art
performance on fine-grained and high-resolution perception. SwimBird obtains 85.5 on V* Bench,
79.0 on HR-Bench 4K, and 74.9 on HR-Bench 8K, outperforming strong textual reasoning baselines
such as Qwen3-VL-8B-Instruct (83.8/76.5/71.3). Notably, Qwen3-VL-Thinking performs worse than
Qwen3-VL-Instruct on visual perception, further supporting our claim that a mismatched reasoning
mode can harm performance. Furthermore, SwimBird also outperforms current state-of-the-art
multimodal agentic models such as Thyme (82.2/77.0/72.0) and DeepEyesV2 (81.8/77.9/73.8), which
enhance perception via explicit cropping tools, highlighting that SwimBird can achieve stronger
fine-grained perception without relying on complex tool pipelines. We attribute these gains to
SwimBird’s query-adaptive reasoning mode switching and adaptive latent-token allocation. Fine-
grained visual tasks often require precise spatial evidence that is difficult to faithfully compress into
text; meanwhile, forcing latent visual thoughts on text-centric steps can be redundant. By switching
to vision-only reasoning when dense perception is needed (and allocating more latent computation
for high-resolution inputs), SwimBird better preserves visual details and reduces modality mismatch,
leading to consistently higher accuracy.
General VQA and Multimodal Reasoning Beyond perception, SwimBird also shows strong
improvements on general VQA and reasoning-heavy benchmarks. As shown in Table 3, SwimBird
reaches 71.2 on MMStar and 73.1 on RealWorldQA, exceeding Qwen3-VL-8B-Instruct* (64.7/71.8)
and even outperforming Qwen2.5-VL-32B-Instruct on MMStar. More importantly, SwimBird
delivers clear gains on multimodal reasoning: 49.5 on WeMath, 67.2 on DynaMath, and 65.8 on
MathVerse_MINI, outperforming strong open-source methods and agentic models. These results
suggest that SwimBird’s latent visual thoughts do not come at the cost of symbolic reasoning. Instead,
SwimBird stays in text-only reasoning when the task is primarily linguistic or mathematical, and
invokes vision-only or interleaved latent thinking only when additional visual evidence is beneficial.
Learned from the multi-pattern supervision in SwimBird-SFT-92K, this query-adaptive selection
avoids redundant visual thoughts that could interfere with textual logic, while still leveraging latent
visual computation for vision-dependent subproblems.
7


===== PAGE 8 =====
Latent Tokens
HRBench4K
HRBench8K
RealWorldQA
16
76.4
71.4
73.1
32
79.0
74.9
73.1
64
77.8
73.4
72.7
128
76.0
71.8
72.7
Table 4: Impact of maximum latent tokens budget.
MSE Weight
HRBench4K
HRBench8K
RealWorldQA
0.1
79.0
71.8
72.8
0.2
79.0
74.9
73.1
0.5
77.8
75.9
72.0
1.0
79.4
73.8
71.9
Table 5: Impact of MSE loss weight coefficients.
Text Only
Vision Only
Interleave
Text Only
Vision Only
Interleave
DynaMath
MathVerse_MINI
V* Bench
WeMath
HR-Bench 4K
HR-Bench 8K
Multimodal Reasoning
Visual Perception
Figure 4: Distribution of reasoning mode across different benchmarks for SwimBird.
4.2
Ablation Studies
Impact of the Maximum Latent Token Budget. We study how the maximum latent token budget
Nmax influences performance under our dynamic range setting [Nmin, Nmax]. We fix Nmin = 2
to ensure small images can be encoded without losing effective resolution, and vary Nmax ∈
{16, 32, 64, 128}. As shown in Table 4, increasing Nmax from 16 to 32 yields clear gains on
vision-dense benchmarks (HRBench4K: 76.4 vs. 79.0; HRBench8K: 71.4 vs. 74.9), indicating that a
moderate upper bound provides sufficient capacity for high-resolution perception. However, further
expanding Nmax to 64 or 128 does not help and even degrades performance (e.g., HRBench8K: 74.9
vs. 73.4 vs. 71.8), while RealWorldQA slightly drops (73.1 vs. 72.7). This suggests that an overly
large latent budget may introduce redundant visual computation and interfere with overall reasoning.
Overall, Nmax = 32 offers the best trade-off and is used as the default setting.
Impact of the MSE Loss Weight Coefficient. We ablate the weight of the visual-thought recon-
struction loss by varying λvis while keeping other settings fixed. As shown in Table 5, a moderate
MSE weight yields the most balanced performance. Specifically, setting λvis = 0.2 achieves strong
results across all benchmarks. When λvis is too small (0.1), the supervision on latent visual thoughts
becomes weak, leading to a notable drop on the most vision-dense benchmark (HRBench8K: 71.8).
In contrast, increasing λvis to 0.5 improves HRBench8K (75.9) but degrades RealWorldQA (72.0),
suggesting that overly emphasizing MSE training may bias the model toward visual reconstruction at
the expense of general multimodal reasoning. With λvis = 1.0, HRBench4K slightly increases (79.4)
but performance drops on HRBench8K and RealWorldQA, indicating instability under overly strong
visual-loss weighting. Overall, we use λvis = 0.2 as the default, which best balances visual reasoning
and text-centric reasoning.
8


===== PAGE 9 =====
Vi
What's the phone number written on the boat?
A. 0905 87 38 28
B. 0905 87 38 38
C. 0905 87 39 38.   D. 0905 87 36 38
<reason>First, I will locate the boat in the image and focus on the 
text written on it, as the phone number is likely printed there. The 
boat in the background has visible text along its side, which needs 
to be closely examined. To highlight the relevant area, I will draw 
a bounding box around the boat's side where the phone number 
is written, making it easier to read the digits.</reason>
<|latent_start|>latent embedding<|latent_end|>
<reason>With the phone number now clearly highlighted, I can 
read the digits written on the boat. The number is "0905 87 38 
38" which matches one of the options. Comparing this with the 
given choices, the correct option is B.</reason>
<answer>B</answer>
Dynamic Length:  Embedding N = 24 Here
<|latent_start|>latent embedding<|latent_end|>
<answer>B</answer>
The outer surface of the carton is displayed on 
the left, and you need to determine which of 
the following options can be folded from it.
A. Picture A
B. Picture B
C. Picture C
D. Picture D
Dynamic Length:  Embedding N = 18 Here
Find the value of 
the black square.
<reason>We are given the equation: □+ 66 = 33 + 60
First, compute the right-hand side: 33 + 60 = 93
So the equation becomes:  □+ 66 = 93
Now subtract 66 from both sides to solve for □:  
□= 93 - 66  
□= 27
Therefore, the value of the black square is 27.</reason>
<answer>27</answer>
Answer: 27
Vision Only Reasoning
Text Only Reasoning
Interleave Vision-Text Reasoning
Figure 5: Analysis of Different Reasoning-Mode Case.
4.3
Analysis of Switchable Reasoning Mode
Analysis of Reasoning-Mode Distribution We analyze the distribution of SwimBird’s reasoning
modes across benchmarks (Fig. 4) to verify its query-adaptive behavior. Overall, the selected mode
matches each benchmark’s dominant difficulty. On text-logic-dominant multimodal reasoning
datasets (DynaMath and MathVerse_MINI), SwimBird almost always uses text-only reasoning,
with vision-only and interleaved traces rarely triggered, suggesting it avoids redundant latent visual
thoughts when symbolic manipulation and linguistic deduction are sufficient. On vision-dense
perception benchmarks (V* Bench and HR-Bench 4K/8K), SwimBird frequently activates vision-
only and especially interleaved vision–text reasoning, reflecting the need to alternate between visual
grounding (e.g., tiny targets in high-resolution images) and explicit textual deduction. The proportion
of vision-only reasoning increases from HR-Bench 4K to 8K, consistent with higher perceptual
load at higher resolutions. WeMath exhibits a more balanced mixture of all three modes, where
some problems are text-centric while others require substantial visual grounding. These results
confirm that SwimBird does not follow a fixed template, but instead selects reasoning modes in an
instance-dependent manner to mitigate modality mismatch.
Analysis of Different Reasoning-Mode Cases Fig. 5 provides qualitative examples of SwimBird’s
mode selection. For vision-only reasoning (top-left), the cube-net folding problem mainly requires
spatial perception and mental rotation; SwimBird directly enters a latent visual-thought span and
outputs the answer without unnecessary textual CoT, while allocating an appropriate latent length (e.g.,
N=18). For text-only reasoning (top-right), the arithmetic equation is purely symbolic; SwimBird
solves it with textual deduction, avoiding redundant visual thoughts that could interfere with logical
steps. For interleaved vision–text reasoning (bottom), reading a phone number from a small region in a
natural image requires both precise visual localization and explicit option comparison; SwimBird first
uses latent visual thoughts to focus on the relevant region, then switches back to text for verification
and decision making, again with a dynamically allocated latent length (e.g., N=24). Together, these
cases show that SwimBird mitigates modality mismatch by choosing when to think in vision versus
language and by adaptively allocating visual-thought computation to match perceptual difficulty.
9


===== PAGE 10 =====
5
Prompt
To guide SwimBird’s query-adaptive reasoning mode selection, we design a system prompt that
explicitly instructs the model on how to switch between textual and visual thinking modes. As shown
in Figure 6, the prompt defines three reasoning patterns: text-only, vision-only, and interleaved, using
structured tags (<reason> for textual thoughts and <latent> for visual thoughts), and allows the
model to dynamically choose the most appropriate mode or combination based on the input query.
System Message
You are a multimodal reasoning assistant capable of thinking in textual and visual modes.
Use the following tags to switch your thinking mode:
1. Textual Mode: <reason>Your textual reasoning process</reason>
• For logical analysis, planning, and verbal thought.
2. Visual Mode: <|latent_start|>Your visual reasoning process<|latent_end|>
• For mental visualization, imagination and simulation.
Output Rules:
• Depending on the problem, you can use: textual reasoning only, visual reasoning only, or a mix of both (alternating multiple times
as needed).
• After all thinking is complete, place the final answer inside <answer>Your Final Answer</answer>.
Figure 6: System prompt used for SwimBird.
6
Conclusion
We present SwimBird, a reasoning-switchable MLLM that addresses the fixed reasoning pattern
in prior multimodal CoT frameworks. SwimBird adopts a hybrid autoregressive paradigm and
can adaptively switch among text-only, vision-only, and interleaved vision–text reasoning, while
dynamically allocating the latent visual token budget. We also construct SwimBird-SFT-92K with a
systematic curation and mode-labeling strategy to enable effective multi-mode training. Extensive
experiments show that SwimBird achieves SoTA performance on both text-centric reasoning and
challenging vision-dense tasks.
References
[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie
Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025.
[2] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang,
Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances
in Neural Information Processing Systems, 37:27056–27087, 2024.
[3] Nigel Dsouza. Comparative analysis of leading generative ai conversational systems: Chatgpt, grok ai,
gemini, and meta ai. Authorea Preprints, 2025.
[4] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith,
Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In
European Conference on Computer Vision, pages 148–166. Springer, 2024.
[5] Jiawei Gu, Yunzhuo Hao, Huichen Will Wang, Linjie Li, Michael Qizhe Shieh, Yejin Choi, Ranjay Krishna,
and Yu Cheng. Thinkmorph: Emergent properties in multimodal interleaved chain-of-thought reasoning.
arXiv preprint arXiv:2510.27492, 2025.
[6] Jack Hong, Chenxiao Zhao, ChengLin Zhu, Weiheng Lu, Guohai Xu, and Xing Yu. Deepeyesv2: Toward
agentic multimodal model. arXiv preprint arXiv:2511.05271, 2025.
[7] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and
Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv
preprint arXiv:2503.06749, 2025.
[8] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,
Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276,
2024.
10


===== PAGE 11 =====
[9] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language
models are zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213,
2022.
[10] Ang Li, Charles Wang, Deqing Fu, Kaiyu Yue, Zikui Cai, Wang Bill Zhu, Ollie Liu, Peng Guo, Willie
Neiswanger, Furong Huang, et al. Zebra-cot: A dataset for interleaved vision language reasoning. arXiv
preprint arXiv:2507.16746, 2025.
[11] Bangzheng Li, Ximeng Sun, Jiang Liu, Ze Wang, Jialian Wu, Xiaodong Yu, Hao Chen, Emad Barsoum,
Muhao Chen, and Zicheng Liu. Latent visual reasoning. arXiv preprint arXiv:2509.24251, 2025.
[12] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang,
Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326,
2024.
[13] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training
with frozen image encoders and large language models. In International conference on machine learning,
pages 19730–19742. PMLR, 2023.
[14] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next:
Improved reasoning, ocr, and world knowledge, January 2024.
[15] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural
information processing systems, 36:34892–34916, 2023.
[16] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang.
Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025.
[17] Runqi Qiao, Qiuna Tan, Guanting Dong, MinhuiWu MinhuiWu, Chong Sun, Xiaoshuai Song, Jiapeng
Wang, Zhuoma Gongque, Shanglin Lei, Yifan Zhang, et al. We-math: Does your large multimodal model
achieve human-like mathematical reasoning? In Proceedings of the 63rd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pages 20023–20070, 2025.
[18] Yiming Qin, Bomin Wei, Jiaxin Ge, Konstantinos Kallidromitis, Stephanie Fu, Trevor Darrell, and XuDong
Wang. Chain-of-visual-thought: Teaching vlms to see and think better with continuous visual tokens. arXiv
preprint arXiv:2511.19418, 2025.
[19] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang,
Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: A stable and generalizable r1-style large vision-language
model. arXiv preprint arXiv:2504.07615, 2025.
[20] Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Mingwei Zhu, and Jianwei
Yin. Zoomeye: Enhancing multimodal llms with human-like zooming capabilities through tree-based
image exploration. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language
Processing, pages 6613–6629, 2025.
[21] Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu,
Yuxuan Hu, Zimu Lu, et al. Mathcanvas: Intrinsic visual chain-of-thought for multimodal mathematical
reasoning. arXiv preprint arXiv:2510.14958, 2025.
[22] Jintao Tong, Jiaqi Gu, Yujing Lou, Lubin Fan, Yixiong Zou, Yue Wu, Jieping Ye, and Ruixuan Li.
Sketch-in-latents: Eliciting unified reasoning in mllms. arXiv preprint arXiv:2512.16584, 2025.
[23] Jintao Tong, Wenwei Jin, Pengda Qin, Anqi Li, Yixiong Zou, Yuhong Li, Yuhua Li, and Ruixuan Li.
Flowcut: Rethinking redundancy via information flow for efficient vision-language models. arXiv preprint
arXiv:2505.19536, 2025.
[24] Jintao Tong, Shiwei Li, Zijian Zhuang, Jinghan Hu, and Yixiong Zou. Emosync: Multi-stage reasoning
with multimodal large language models for fine-grained emotion recognition. In Proceedings of the 3rd
International Workshop on Multimodal and Responsible Affective Computing, pages 95–99, 2025.
[25] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker:
Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint
arXiv:2504.08837, 2025.
[26] Haozhe Wang, Alex Su, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing
pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966,
2025.
11


===== PAGE 12 =====
[27] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin
Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any
resolution. arXiv preprint arXiv:2409.12191, 2024.
[28] Qixun Wang, Yang Shi, Yifei Wang, Yuanxing Zhang, Pengfei Wan, Kun Gai, Xianghua Ying, and
Yisen Wang. Monet: Reasoning in latent visual space beyond images and language. arXiv preprint
arXiv:2511.21395, 2025.
[29] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin
Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility,
reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025.
[30] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng
Tao. Divide, conquer and combine: A training-free framework for high-resolution image perception in
multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 39, pages 7907–7915, 2025.
[31] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu,
Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal
llms. Advances in Neural Information Processing Systems, 37:113569–113697, 2024.
[32] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural
information processing systems, 35:24824–24837, 2022.
[33] Penghao Wu and Saining Xie. V?: Guided visual search as a core mechanism in multimodal llms. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13084–
13094, 2024.
[34] Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision
language models reason step-by-step. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 2087–2098, 2025.
[35] Shilin Yan, Jiaming Han, Joey Tsai, Hongwei Xue, Rongyao Fang, Lingyi Hong, Ziyu Guo, and Ray
Zhang. Crosslmm: Decoupling long video sequences from lmms via dual cross-attention mechanisms.
arXiv preprint arXiv:2505.17020, 2025.
[36] Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. Machine mental imagery:
Empower multimodal reasoning with latent visual tokens. arXiv preprint arXiv:2506.17218, 2025.
[37] En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui
Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv
preprint arXiv:2504.07954, 2025.
[38] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu
Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding
and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 9556–9567, 2024.
[39] Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong,
Zhang Zhang, Liang Wang, et al. Latent sketchpad: Sketching visual thoughts to elicit multimodal
reasoning in mllms. arXiv preprint arXiv:2510.24514, 2025.
[40] Kaichen Zhang, Keming Wu, Zuhao Yang, Bo Li, Kairui Hu, Bin Wang, Ziwei Liu, Xingxuan Li, and
Lidong Bing. Openmmreasoner: Pushing the frontiers for multimodal reasoning with an open and general
recipe. arXiv preprint arXiv:2511.16334, 2025.
[41] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu,
Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual
math problems? In European Conference on Computer Vision, pages 169–186. Springer, 2024.
[42] Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi
Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv preprint arXiv:2508.11630, 2025.
[43] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li,
Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge
high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024.
12


===== PAGE 13 =====
[44] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal
chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023.
[45] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and
Xing Yu. Deepeyes: Incentivizing" thinking with images" via reinforcement learning. arXiv preprint
arXiv:2505.14362, 2025.
[46] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan,
Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source
multimodal models. arXiv preprint arXiv:2504.10479, 2025.
[47] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: A dynamic
visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv
preprint arXiv:2411.00836, 2024.
13
