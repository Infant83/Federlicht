- **[메타데이터/커버리지 점검(인덱스 파일)]**
  - 본 런 아카이브에는 arXiv PDF 10편과 추출 텍스트 10개, 그리고 Tavily Extract로 저장된 NVIDIA Glossary 1개만 존재합니다. 즉, “웹서치/산업 동향”은 실질적으로 NVIDIA Glossary 1개 외에는 추가 근거가 없습니다. [archive/physical_ai_insight-index.md]
  - Supporting 폴더(추가 웹근거 업데이트 경로)가 없어서, supporting 기반의 최신 웹 근거 반영은 불가능합니다. (시스템 응답: “Supporting folder not available.”) [archive/physical_ai_insight-index.md]

- **[웹(정의 앵커) — NVIDIA Glossary]**
  - NVIDIA Glossary 페이지는 “What is Physical AI?”라는 제목의 항목으로 존재하며 URL은 `https://www.nvidia.com/en-us/glossary/generative-physical-ai/` 입니다. 보고서의 “개념/범위” 섹션에서 Physical AI 정의를 직접 인용할 1차 근거(유일한 웹 근거)입니다. https://www.nvidia.com/en-us/glossary/generative-physical-ai/ [archive/tavily_extract/0001_https_www.nvidia.com_en-us_glossary_generative-physical-ai.txt]

- **[arXiv 논문(Primary evidence) — VLA/Generalist policy]**
  - **Octo (arXiv:2405.12213v2)**: “open-source, generalist policy for robotic manipulation”을 제시하며, Open X-Embodiment 데이터의 **800k diverse robot episodes/trajectories**로 사전학습한 **transformer-based policy**이고, 언어 지시 또는 목표 이미지로 지시 가능하며, “표준 consumer GPU에서 몇 시간 내” 새로운 관측/액션 공간으로 파인튜닝 가능하다고 명시합니다. https://arxiv.org/pdf/2405.12213v2 [archive/arxiv/text/2405.12213v2.txt]
  - **OpenVLA (arXiv:2406.09246v3)**: **7B 파라미터** 오픈소스 VLA로, Open X-Embodiment의 **970k robot episodes**로 학습했다고 밝힙니다. 또한 **RT-2-X(55B) 대비 29개 태스크·복수 embodiment에서 성공률이 절대값 16.5%p 높고 파라미터는 7배 적다**고 주장하며, 양자화로 서빙 효율을 높이되 다운스트림 성공률 손실 없이 가능하다는 서술이 있습니다. https://arxiv.org/pdf/2406.09246v3 [archive/arxiv/text/2406.09246v3.txt]
  - **π0 (arXiv:2410.24164v4)**: 사전학습 VLM 백본 위에 **flow matching 기반의 action expert**를 추가해 연속 액션을 생성한다고 설명합니다. 또한 **7개 로봇 구성(7 distinct robot configurations), 68개 태스크**에 대한 프리트레이닝을 언급하며, 프롬프트 기반 수행 및 파인튜닝을 모두 다룹니다. https://arxiv.org/pdf/2410.24164v4 [archive/arxiv/text/2410.24164v4.txt]
  - **CogACT (arXiv:2411.19650v1)**: VLM을 단순 액션 토큰화로 쓰는 대신, VLM 출력에 조건부인 **specialized action module**을 분리한 “componentized VLA”를 제안하고, 액션 모듈로 **diffusion-based transformers(DiT)**를 사용한다고 명시합니다. 또한 OpenVLA(유사 7B 크기) 대비 **시뮬레이션 평균 성공률 +35% 이상, 실로봇 +55%**, RT-2-X 대비 시뮬레이션에서 **절대 성공률 +18%p**를 주장하며 “Code and models” 공개를 언급합니다. https://arxiv.org/pdf/2411.19650v1 [archive/arxiv/text/2411.19650v1.txt]
  - **Towards Generalist Robot Policies / RoboVLMs (arXiv:2412.14058v3)**: VLA 설계에서 “어떤 백본/아키텍처/언제 cross-embodiment data를 넣는가” 등 **핵심 설계요인**을 체계적으로 분석한다고 밝히며, **8개 이상 VLM 백본, 4개 policy architectures, 600개 이상 실험**을 포함한 “guidebook”을 표방합니다. 또한 코드/모델/데이터/레시피를 공개한다고 명시합니다. https://arxiv.org/pdf/2412.14058v3 [archive/arxiv/text/2412.14058v3.txt]

- **[arXiv 논문(Primary evidence) — Action chunking/실시간 실행·서빙]**
  - **PD‑VLA (arXiv:2503.02310v1)**: action chunking이 chunk 크기 m에서 **7DoF 팔 기준 7m 차원의 시퀀스**로 늘며 AR decoding에서는 토큰 길이에 비례해 지연이 증가한다고 지적합니다. 이를 해결하기 위해 **parallel decoding**(AR 디코딩을 비선형 시스템으로 재정식화, fixed-point iteration으로 병렬 해) 프레임워크를 제안하고, **7DoF 매니퓰레이터에서 기본 VLA 대비 2.52× execution frequency**를 보고합니다(시뮬레이션). https://arxiv.org/pdf/2503.02310v1 [archive/arxiv/text/2503.02310v1.txt]
  - **RTC (arXiv:2506.07339v2)**: 큰 모델 지연이 물리 시스템에서 성능/안전에 직접 영향을 준다고 강조하며(“coffee를 쏟을 수도 있다”는 대비), action chunking만으로는 chunk boundary에서 “pauses/jerky movements” 문제가 남는다고 서술합니다. **RTC(real-time chunking)**는 현재 chunk를 실행하는 동안 다음 chunk를 생성하는 비동기 실행을 제안하며, **300ms 초과 지연(모델 예측 horizon의 30%+에 해당) 상황에서도** 정밀 태스크(예: 성냥 켜기)에서 높은 성공률을 가능케 했다고 설명합니다. https://arxiv.org/pdf/2506.07339v2 [archive/arxiv/text/2506.07339v2.txt]

- **[arXiv 논문(Primary evidence) — 휴머노이드/Physical AI 확장]**
  - **GR00T N1 (arXiv:2503.14734v2)**: 휴머노이드용 오픈 foundation model로, **dual-system architecture**를 제시합니다. System 2(VLM)는 **NVIDIA L40에서 10Hz**로 추론하고, System 1은 **diffusion transformer(+ action flow-matching)**로 **120Hz** 폐루프 모터 액션을 생성한다고 명시합니다. 또한 학습 데이터로 **real-robot trajectories + human videos + synthetic datasets**의 혼합을 사용한다고 밝힙니다. https://arxiv.org/pdf/2503.14734v2 [archive/arxiv/text/2503.14734v2.txt]
  - **Gemini Robotics (arXiv:2503.20020v1)**: Gemini 2.0 기반의 로보틱스 특화 모델 패밀리로 **Gemini Robotics(VLA로 로봇 직접 제어)**와 **Gemini Robotics‑ER(Embodied Reasoning)**를 소개합니다. 추가 파인튜닝으로 종이접기/카드게임 같은 장기·고난도 태스크, **100 demonstrations만으로 새로운 short-horizon 태스크 학습**, 그리고 **새로운 로봇 embodiment(바이암, high-DoF humanoid)** 적응을 주장하며, “important safety considerations”를 다룬다고 명시합니다. https://arxiv.org/pdf/2503.20020v1 [archive/arxiv/text/2503.20020v1.txt]

- **[arXiv 논문(Primary evidence) — 압축/양자화·엣지 제약]**
  - **BitVLA (arXiv:2506.07530v1)**: 모든 파라미터를 ternary({−1,0,1})로 제한한 **“first 1-bit VLA”**를 주장하고, 비전 인코더 메모리 풋프린트를 줄이기 위해 **distillation-aware training으로 1.58-bit weights**로 압축한다고 설명합니다. 또한 LIBERO 벤치마크에서 OpenVLA-OFT(INT4)와 **유사 성능**을 보이면서 **메모리는 29.8%만 사용**했다고 보고하며, 코드/가중치 공개 GitHub를 제공합니다. https://arxiv.org/pdf/2506.07530v1 [archive/arxiv/text/2506.07530v1.txt]