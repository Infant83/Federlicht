{
  "content": "## 1) \uc544\uce74\uc774\ube0c \ub9f5(coverage) \uc694\uc57d\n\n### \ud575\uc2ec \uc778\ub371\uc2a4/\uba54\ud0c0\ub370\uc774\ud130(\u201c\uc18c\uc2a4 \uc778\ub371\uc2a4\u201d\ub85c\ub9cc \ucde8\uae09)\n- `archive/physical_ai_insight-index.md` : \uc774 \ub7f0(run)\uc758 \uc804\uccb4 \uad6c\uc131 \uc694\uc57d(\uc218\uc9d1 \ubc94\uc704, \ud30c\uc77c \ubaa9\ub85d)\n- `archive/arxiv/papers.jsonl` : arXiv 10\ud3b8 \uba54\ud0c0\ub370\uc774\ud130(\uc81c\ubaa9/\uc694\uc57d/URL/PDF/text \uacbd\ub85c)\n- `report_notes/source_index.jsonl` : \uc2e4\uc81c \ubcf4\uace0\uc11c\uc6a9\uc73c\ub85c \uc120\ubcc4\ub41c \uc18c\uc2a4 id\u2194\uacbd\ub85c \ub9e4\ud551\n- `report_notes/source_triage.md` : \uacbd\ub7c9 \ud2b8\ub9ac\uc544\uc9c0(\uc810\uc218/\uc5f0\ub3c4/\ud14d\uc2a4\ud2b8 \uacbd\ub85c)\n\n### \uc2e4\uc81c \ucf58\ud150\uce20(\uc77d\uc744 \ubcf8\ubb38)\n- arXiv PDF: `archive/arxiv/pdf/*.pdf` (10\uac1c)\n- arXiv \ud14d\uc2a4\ud2b8 \ucd94\ucd9c: `archive/arxiv/text/*.txt` (10\uac1c)\n- Tavily extract(\uc6f9\ud398\uc774\uc9c0 \ub364\ud504):  \n  - `archive/tavily_extract/0001_https_www.nvidia.com_en-us_glossary_generative-physical-ai.txt` (NVIDIA \u201cWhat is Physical AI?\u201d)\n\n### \uc774\ubc88 \ub7f0\uc5d0\uc11c \u201c\uc5c6\ub294\u201d \uc778\ub371\uc2a4(\ud655\uc778 \uacb0\uacfc)\n- `archive/tavily_search.jsonl` \uc5c6\uc74c\n- `archive/openalex/works.jsonl` \uc5c6\uc74c\n- `archive/youtube/videos.jsonl` \uc5c6\uc74c\n- `archive/local/manifest.jsonl` \uc5c6\uc74c\n- `instruction/physical_ai_insight.txt` \ub294 \uc874\uc7ac\ud558\uc9c0\ub9cc \u201csupporting folder\u201d \ud234 \uacbd\ub85c\ub85c\ub294 \uc811\uadfc \ubd88\uac00 \u2192 `read_document`\ub85c \ub0b4\uc6a9 \ud655\uc778 \uc644\ub8cc(\ub9c1\ud06c \ub9ac\uc2a4\ud2b8 \ud615\ud0dc)\n\n---\n\n## 2) \uad6c\uc870\ud654 \uc778\ubca4\ud1a0\ub9ac(\uc8fc\uc81c\ubcc4 \ud074\ub7ec\uc2a4\ud130)\n\n### A. \uac1c\ub150/\uc6a9\uc5b4(Physical AI \uc815\uc758 \ud504\ub808\uc774\ubc0d)\n- NVIDIA Glossary: **\u201cWhat is Physical AI?\u201d**  \n  - \uacbd\ub85c: `archive/tavily_extract/0001_https_www.nvidia.com_en-us_glossary_generative-physical-ai.txt`\n\n### B. Generalist Robot Policy / VLA \uae30\ucd08 \ubaa8\ub378(\ub300\ud45c \ubca0\uc774\uc2a4\ub77c\uc778)\n- **Octo: An Open-Source Generalist Robot Policy** (arXiv:2405.12213v2)  \n  - text: `archive/arxiv/text/2405.12213v2.txt` / pdf: `archive/arxiv/pdf/2405.12213v2.pdf`\n- **OpenVLA: An Open-Source Vision-Language-Action Model** (arXiv:2406.09246v3)  \n  - text: `archive/arxiv/text/2406.09246v3.txt` / pdf: `archive/arxiv/pdf/2406.09246v3.pdf`\n\n### C. \uc544\ud0a4\ud14d\ucc98 \ud2b8\ub80c\ub4dc(Flow/Diffusion, \u201c\u03c00\u201d \uacc4\uc5f4)\n- **$\u03c0_0$: A Vision-Language-Action Flow Model for General Robot Control** (arXiv:2410.24164v4)  \n  - text: `archive/arxiv/text/2410.24164v4.txt` / pdf: `archive/arxiv/pdf/2410.24164v4.pdf`\n\n### D. \uc131\ub2a5/\uc124\uacc4 \uc120\ud0dd \ubd84\uc11d(\u201c\ubb34\uc5c7\uc774 \uc911\uc694\ud55c\uac00\u201d \uac00\uc774\ub4dc)\n- **Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models** (arXiv:2412.14058v3; RoboVLMs)  \n  - text: `archive/arxiv/text/2412.14058v3.txt` / pdf: `archive/arxiv/pdf/2412.14058v3.pdf`\n\n### E. \uc561\uc158 \ubaa8\ub4c8/\ucef4\ud3ec\ub10c\ud2b8\ud654(\uc778\uc9c0-\ud589\ub3d9 \uacb0\ud569 \uac15\ud654)\n- **CogACT: A Foundational Vision-Language-Action Model\u2026** (arXiv:2411.19650v1)  \n  - text: `archive/arxiv/text/2411.19650v1.txt` / pdf: `archive/arxiv/pdf/2411.19650v1.pdf`\n\n### F. \ucd5c\uc2e0(2025) \uc2dc\uc2a4\ud15c/\ud734\uba38\ub178\uc774\ub4dc/\uc2e4\ud589 \uac00\uc18d(Production/Real-time)\n- **Gemini Robotics: Bringing AI into the Physical World** (arXiv:2503.20020v1)  \n  - text: `archive/arxiv/text/2503.20020v1.txt` / pdf: `archive/arxiv/pdf/2503.20020v1.pdf`\n- **GR00T N1: An Open Foundation Model for Generalist Humanoid Robots** (arXiv:2503.14734v2)  \n  - text: `archive/arxiv/text/2503.14734v2.txt` / pdf: `archive/arxiv/pdf/2503.14734v2.pdf`\n- **Accelerating VLA + Action Chunking via Parallel Decoding** (arXiv:2503.02310v1; PD-VLA)  \n  - text: `archive/arxiv/text/2503.02310v1.txt` / pdf: `archive/arxiv/pdf/2503.02310v1.pdf`\n- **Real-Time Execution of Action Chunking Flow Policies** (arXiv:2506.07339v2; RTC)  \n  - text: `archive/arxiv/text/2506.07339v2.txt` / pdf: `archive/arxiv/pdf/2506.07339v2.pdf`\n- **BitVLA: 1-bit Vision-Language-Action Models\u2026** (arXiv:2506.07530v1)  \n  - text: `archive/arxiv/text/2506.07530v1.txt` / pdf: `archive/arxiv/pdf/2506.07530v1.pdf`\n\n---\n\n## 3) \uc6b0\uc120 \uc77d\uae30 \ubaa9\ub85d(\ucd5c\ub300 12\uac1c) + \uc120\uc815 \uc774\uc720\n\n1) **Gemini Robotics: Bringing AI into the Physical World** (`2503.20020v1`, txt/pdf)  \n- \uc774\uc720: \u201cPhysical AI\u201d\ub97c \ub85c\ubcf4\ud2f1\uc2a4 \uc2dc\uc2a4\ud15c \ub808\ubca8\ub85c \ud655\uc7a5\ud558\ub294 \ub300\ud45c \ucd5c\uc2e0 \ucd95(2025). \uc804\uccb4 \ubcf4\uace0\uc11c\uc758 \ubc29\ud5a5\ud0a4(\ubb38\uc81c\uc815\uc758/\uc2a4\ud0dd/\ud3c9\uac00)\ub97c \uc81c\uacf5\ud560 \uac00\ub2a5\uc131\uc774 \ud07c.\n\n2) **OpenVLA: An Open-Source Vision-Language-Action Model** (`2406.09246v3`, txt/pdf)  \n- \uc774\uc720: \uc624\ud508 VLA\uc758 \uc0ac\uc2e4\uc0c1 \ud45c\uc900\uae09 \ub808\ud37c\ub7f0\uc2a4(\ub370\uc774\ud130/\uc544\ud0a4\ud14d\ucc98/\ud30c\uc778\ud29c\ub2dd/\uc11c\ube59 \ud6a8\uc728). \ud6c4\uc18d(\uac00\uc18d/\uc555\ucd95/\uc561\uc158\ubaa8\ub4c8) \ub17c\ubb38\ub4e4\uc744 \ube44\uad50\ud558\ub294 \uae30\uc900\uc810.\n\n3) **Octo: An Open-Source Generalist Robot Policy** (`2405.12213v2`, txt/pdf)  \n- \uc774\uc720: \u201cgeneralist policy\u201d \uacc4\uc5f4\uc758 \ud575\uc2ec \ubca0\uc774\uc2a4\ub77c\uc778. Open X-Embodiment/\ub2e4\uc911 \ud50c\ub7ab\ud3fc/\uc561\uc158-\uad00\uce21 \ub2e4\uc591\uc131 \ub300\uc751\uc774 Physical AI \ubb38\ub9e5\uc5d0\uc11c \uc911\uc694.\n\n4) **Towards Generalist Robot Policies: What Matters in Building VLA Models (RoboVLMs)** (`2412.14058v3`, txt/pdf)  \n- \uc774\uc720: \uc124\uacc4 \uc120\ud0dd(\ubc31\ubcf8/\uc544\ud0a4\ud14d\ucc98/\ud06c\ub85c\uc2a4-\uc5e0\ubcf4\ub514\uba3c\ud2b8 \ub370\uc774\ud130 \ud22c\uc785 \uc2dc\uc810)\uc744 \uccb4\uacc4\uc801\uc73c\ub85c \uc815\ub9ac\ud55c \u201c\uac00\uc774\ub4dc\ubd81\u201d \uc131\uaca9. \uc778\uc0ac\uc774\ud2b8 \ucd94\ucd9c\uc5d0 \ucd5c\uc801.\n\n5) **$\u03c0_0$: A Vision-Language-Action Flow Model for General Robot Control** (`2410.24164v4`, txt/pdf)  \n- \uc774\uc720: Flow matching \uae30\ubc18 VLA\ub77c\ub294 \uc544\ud0a4\ud14d\ucc98 \ud2b8\ub80c\ub4dc \ucd95. \u201cpolicy formulation\u201d \ube44\uad50(\ub514\ud4e8\uc804/\ud50c\ub85c\uc6b0/\ud1a0\ud070\ud654 \ub4f1) \uad00\uc810\uc5d0 \ud544\uc694.\n\n6) **CogACT: A Foundational Vision-Language-Action Model\u2026** (`2411.19650v1`, txt/pdf)  \n- \uc774\uc720: VLM\u2192VLA \uc804\ud658\uc5d0\uc11c \u201c\ucef4\ud3ec\ub10c\ud2b8\ud654\ub41c action module\u201d\uc744 \uac15\uc870. \uc131\ub2a5 \ud5a5\uc0c1 \uc8fc\uc7a5(\ud2b9\ud788 OpenVLA/RT-2-X \ub300\ube44)\uc744 \uac80\uc99d \ud3ec\uc778\ud2b8\ub85c \uc0bc\uae30 \uc88b\uc74c.\n\n7) **GR00T N1: An Open Foundation Model for Generalist Humanoid Robots** (`2503.14734v2`, txt/pdf)  \n- \uc774\uc720: \ud734\uba38\ub178\uc774\ub4dc\ub77c\ub294 \u201cPhysical AI\uc758 \ucd5c\uc885 \ud3fc\ud329\ud130\u201d \ucd95. \ub370\uc774\ud130/\ud559\uc2b5/\uc81c\uc5b4 \uad6c\uc870\uac00 \ub9e4\ub2c8\ud4f0\ub808\uc774\uc158 \uc911\uc2ec VLA\uc640 \uc5b4\ub5bb\uac8c \ub2e4\ub978\uc9c0 \ube44\uad50.\n\n8) **Real-Time Execution of Action Chunking Flow Policies** (`2506.07339v2`, txt/pdf)  \n- \uc774\uc720: \uc2e4\uc81c \ubc30\uce58/\uc2e4\ud589\uc5d0\uc11c \ubcd1\ubaa9\uc778 \u201c\uc2e4\uc2dc\uac04\uc131\u201d\uc744 \uc815\uba74\uc73c\ub85c \ub2e4\ub8f8. \ub370\ubaa8 \uc131\uacf5\ub960 \uc678\uc5d0 latency/throughput \uad00\uc810 \uc778\uc0ac\uc774\ud2b8 \ud655\ubcf4.\n\n9) **Accelerating VLA + Action Chunking via Parallel Decoding (PD-VLA)** (`2503.02310v1`, txt/pdf)  \n- \uc774\uc720: (8)\uacfc \uc9dd\uc744 \uc774\ub8e8\ub294 \u201c\uac00\uc18d/\ub514\ucf54\ub529\u201d \uacc4\uc5f4. action chunking\uc774 VLA \uc11c\ube59\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5 \uc815\ub9ac\uc5d0 \ud544\uc694.\n\n10) **BitVLA: 1-bit Vision-Language-Action Models\u2026** (`2506.07530v1`, txt/pdf)  \n- \uc774\uc720: \uc555\ucd95/\uc591\uc790\ud654(\ucd08\uc800\ube44\ud2b8)\ub85c \uc5e3\uc9c0/\uc2e4\uc2dc\uac04/\ube44\uc6a9 \ubb38\uc81c\ub97c \ub2e4\ub8e8\ub294 \ucd95. Physical AI\uc758 \uc81c\ud488\ud654 \uad00\uc810\uc5d0\uc11c \uc911\uc694.\n\n11) NVIDIA Glossary **\u201cWhat is Physical AI?\u201d** (`archive/tavily_extract/...generative-physical-ai.txt`)  \n- \uc774\uc720: \uc6a9\uc5b4 \ud504\ub808\uc774\ubc0d(\u201cPhysical AI\u201d \uc815\uc758, \ubc94\uc704, \uba54\uc2dc\uc9d5)\uc744 \ubcf4\uace0\uc11c \uc11c\ub860/\uc815\uc758 \ud30c\ud2b8\uc5d0 \ud65c\uc6a9\ud558\uae30 \uc88b\uc74c. (\ub2e8, \uc6f9 \ub364\ud504\uac00 \ub9e4\uc6b0 \uae38\uc5b4 \ud544\uc694\ud55c \uc139\uc158\ub9cc \ubc1c\ucdcc \uc77d\uae30 \uad8c\uc7a5)\n\n12) (\uc120\ud0dd) `archive/arxiv/papers.jsonl`  \n- \uc774\uc720: \uac01 \ub17c\ubb38 \uc694\uc57d/\uba54\ud0c0(\uc5c5\ub370\uc774\ud2b8 \ub0a0\uc9dc, \uce74\ud14c\uace0\ub9ac, \uc778\uc6a9\uc218)\ub97c \ube60\ub974\uac8c \uc2a4\uce94\ud558\uba70 \ubcf4\uace0\uc11c \ubaa9\ucc28 \uc124\uacc4\ub97c \ub3d5\ub294 \u201c\ucee8\ud2b8\ub864 \ud328\ub110\u201d.\n\n---\n\n## 4) \ucd94\ucc9c \uc77d\uae30 \ud50c\ub79c(\ud6a8\uc728 \uc911\uc2ec)\n\n- **1\ud328\uc2a4(\uc2a4\uce90\ub2dd, 60\u201390\ubd84)**: \uac01 \ub17c\ubb38 `text/*.txt`\uc5d0\uc11c Abstract/Intro/Method/Experiments/Limitations \uc139\uc158\ub9cc \ube60\ub974\uac8c \ud6d1\uae30  \n  - \ubaa9\ud45c: \uacf5\ud1b5 \ud0a4\uc6cc\ub4dc(\ub370\uc774\ud130 \uc2a4\ucf00\uc77c, embodiment, action representation, inference speed, fine-tuning recipe) \uccb4\ud06c\ub9ac\uc2a4\ud2b8\ud654\n- **2\ud328\uc2a4(\ub525\ub2e4\uc774\ube0c, 3\u20135\ud3b8)**: OpenVLA / Octo / RoboVLMs / \u03c00 / Gemini Robotics\ub97c PDF\ub85c \uc0c1\uc138(\uadf8\ub9bc, \ud45c, \uc5b4\ube14\ub808\uc774\uc158) \ud655\uc778  \n- **3\ud328\uc2a4(\uc2dc\uc2a4\ud15c \uad00\uc810 \ubcf4\uac15)**: PD-VLA + RTC + BitVLA\ub85c \u201c\uc2e4\uc2dc\uac04/\uc11c\ube59/\uc555\ucd95\u201d \ucd95\uc744 \uc815\ub9ac\ud574, \uc5f0\uad6c\u2192\uc81c\ud488 \uac04\uadf9 \uc778\uc0ac\uc774\ud2b8 \ub3c4\ucd9c\n\n\ud544\uc694\ud558\uba74, \uc704 12\uac1c \uc911\uc5d0\uc11c \u201c\ubcf4\uace0\uc11c \ud3ec\ucee4\uc2a4(\uc608: \uc2e4\uc2dc\uac04 \uc2e4\ud589, \ud734\uba38\ub178\uc774\ub4dc, \uc624\ud508\uc18c\uc2a4 \uc7ac\ud604\uc131, \ub370\uc774\ud130 \ub808\uc2dc\ud53c)\u201d\ub97c \uc9c0\uc815\ud574 \uc8fc\uba74 \uadf8 \ucd95\uc73c\ub85c \uc6b0\uc120\uc21c\uc704\ub97c \uc7ac\uc815\ub82c\ud574 \ub4dc\ub9b4\uac8c\uc694.",
  "meta": {
    "stage": "scout",
    "model": "gpt-5.2",
    "prompt_hash": "8216fd6868df9bd1329f0c6249386892b7a0d8700beedbf57b8e0b46f02764ad",
    "input_hash": "1a49e20aac696b0959bf19e5bf54d42d5d33d2c6756a128c30f6a0ef286fc8c2"
  },
  "created_at": "2026-02-06T06:47:07.533465"
}