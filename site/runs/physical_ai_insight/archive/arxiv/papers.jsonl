{"arxiv_id": "2405.12213v2", "title": "Octo: An Open-Source Generalist Robot Policy", "authors": ["Octo Model Team", "Dibya Ghosh", "Homer Walke", "Karl Pertsch", "Kevin Black", "Oier Mees", "Sudeep Dasari", "Joey Hejna", "Tobias Kreiman", "Charles Xu", "Jianlan Luo", "You Liang Tan", "Lawrence Yunliang Chen", "Pannag Sanketi", "Quan Vuong", "Ted Xiao", "Dorsa Sadigh", "Chelsea Finn", "Sergey Levine"], "published": "2024-05-20T17:57:01+00:00", "updated": "2024-05-26T19:55:26+00:00", "summary": "Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.LG"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2405.12213v2", "entry_id": "http://arxiv.org/abs/2405.12213v2", "cited_by_count": 7}
{"arxiv_id": "2406.09246v3", "title": "OpenVLA: An Open-Source Vision-Language-Action Model", "authors": ["Moo Jin Kim", "Karl Pertsch", "Siddharth Karamcheti", "Ted Xiao", "Ashwin Balakrishna", "Suraj Nair", "Rafael Rafailov", "Ethan Foster", "Grace Lam", "Pannag Sanketi", "Quan Vuong", "Thomas Kollar", "Benjamin Burchfiel", "Russ Tedrake", "Dorsa Sadigh", "Sergey Levine", "Percy Liang", "Chelsea Finn"], "published": "2024-06-13T15:46:55+00:00", "updated": "2024-09-05T19:46:34+00:00", "summary": "Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.LG"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2406.09246v3", "entry_id": "http://arxiv.org/abs/2406.09246v3", "cited_by_count": 34}
{"arxiv_id": "2410.24164v4", "title": "$π_0$: A Vision-Language-Action Flow Model for General Robot Control", "authors": ["Kevin Black", "Noah Brown", "Danny Driess", "Adnan Esmail", "Michael Equi", "Chelsea Finn", "Niccolo Fusai", "Lachy Groom", "Karol Hausman", "Brian Ichter", "Szymon Jakubczak", "Tim Jones", "Liyiming Ke", "Sergey Levine", "Adrian Li-Bell", "Mohith Mothukuri", "Suraj Nair", "Karl Pertsch", "Lucy Xiaoyang Shi", "James Tanner", "Quan Vuong", "Anna Walling", "Haohuan Wang", "Ury Zhilinsky"], "published": "2024-10-31T17:22:30+00:00", "updated": "2026-01-08T17:01:05+00:00", "summary": "Robot learning holds tremendous promise to unlock the full potential of flexible, general, and dexterous robot systems, as well as to address some of the deepest questions in artificial intelligence. However, bringing robot learning to the level of generality required for effective real-world systems faces major obstacles in terms of data, generalization, and robustness. In this paper, we discuss how generalist robot policies (i.e., robot foundation models) can address these challenges, and how we can design effective generalist robot policies for complex and highly dexterous tasks. We propose a novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge. We then discuss how this model can be trained on a large and diverse dataset from multiple dexterous robot platforms, including single-arm robots, dual-arm robots, and mobile manipulators. We evaluate our model in terms of its ability to perform tasks in zero shot after pre-training, follow language instructions from people and from a high-level VLM policy, and its ability to acquire new skills via fine-tuning. Our results cover a wide variety of tasks, such as laundry folding, table cleaning, and assembling boxes.", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.RO"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2410.24164v4", "entry_id": "http://arxiv.org/abs/2410.24164v4", "cited_by_count": 6}
{"arxiv_id": "2411.19650v1", "title": "CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation", "authors": ["Qixiu Li", "Yaobo Liang", "Zeyu Wang", "Lin Luo", "Xi Chen", "Mozheng Liao", "Fangyun Wei", "Yu Deng", "Sicheng Xu", "Yizhong Zhang", "Xiaofan Wang", "Bei Liu", "Jianlong Fu", "Jianmin Bao", "Dong Chen", "Yuanchun Shi", "Jiaolong Yang", "Baining Guo"], "published": "2024-11-29T12:06:03+00:00", "updated": "2024-11-29T12:06:03+00:00", "summary": "The advancement of large Vision-Language-Action (VLA) models has significantly improved robotic manipulation in terms of language-guided task execution and generalization to unseen scenarios. While existing VLAs adapted from pretrained large Vision-Language-Models (VLM) have demonstrated promising generalizability, their task performance is still unsatisfactory as indicated by the low tasks success rates in different environments. In this paper, we present a new advanced VLA architecture derived from VLM. Unlike previous works that directly repurpose VLM for action prediction by simple action quantization, we propose a omponentized VLA architecture that has a specialized action module conditioned on VLM output. We systematically study the design of the action module and demonstrates the strong performance enhancement with diffusion action transformers for action sequence modeling, as well as their favorable scaling behaviors. We also conduct comprehensive experiments and ablation studies to evaluate the efficacy of our models with varied designs. The evaluation on 5 robot embodiments in simulation and real work shows that our model not only significantly surpasses existing VLAs in task performance and but also exhibits remarkable adaptation to new robots and generalization to unseen objects and backgrounds. It exceeds the average success rates of OpenVLA which has similar model size (7B) with ours by over 35% in simulated evaluation and 55% in real robot experiments. It also outperforms the large RT-2-X model (55B) by 18% absolute success rates in simulation. Code and models can be found on our project page (https://cogact.github.io/).", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2411.19650v1", "entry_id": "http://arxiv.org/abs/2411.19650v1", "cited_by_count": 5}
{"arxiv_id": "2412.14058v3", "title": "Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models", "authors": ["Xinghang Li", "Peiyan Li", "Minghuan Liu", "Dong Wang", "Jirong Liu", "Bingyi Kang", "Xiao Ma", "Tao Kong", "Hanbo Zhang", "Huaping Liu"], "published": "2024-12-18T17:07:20+00:00", "updated": "2024-12-24T04:43:45+00:00", "summary": "Foundation Vision Language Models (VLMs) exhibit strong capabilities in multi-modal representation learning, comprehension, and reasoning. By injecting action components into the VLMs, Vision-Language-Action Models (VLAs) can be naturally formed and also show promising performance. Existing work has demonstrated the effectiveness and generalization of VLAs in multiple scenarios and tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since existing VLAs differ in their backbones, action-prediction formulations, data distributions, and training recipes. This leads to a missing piece for a systematic understanding of the design choices of VLAs. In this work, we disclose the key factors that significantly influence the performance of VLA and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures, and when to add cross-embodiment data. The obtained results convince us firmly to explain why we need VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures, and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets, and toolkits, along with detailed training and evaluation recipes at: robovlms.github.io.", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.CV"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2412.14058v3", "entry_id": "http://arxiv.org/abs/2412.14058v3", "cited_by_count": 0}
{"arxiv_id": "2503.20020v1", "title": "Gemini Robotics: Bringing AI into the Physical World", "authors": ["Gemini Robotics Team", "Saminda Abeyruwan", "Joshua Ainslie", "Jean-Baptiste Alayrac", "Montserrat Gonzalez Arenas", "Travis Armstrong", "Ashwin Balakrishna", "Robert Baruch", "Maria Bauza", "Michiel Blokzijl", "Steven Bohez", "Konstantinos Bousmalis", "Anthony Brohan", "Thomas Buschmann", "Arunkumar Byravan", "Serkan Cabi", "Ken Caluwaerts", "Federico Casarini", "Oscar Chang", "Jose Enrique Chen", "Xi Chen", "Hao-Tien Lewis Chiang", "Krzysztof Choromanski", "David D'Ambrosio", "Sudeep Dasari", "Todor Davchev", "Coline Devin", "Norman Di Palo", "Tianli Ding", "Adil Dostmohamed", "Danny Driess", "Yilun Du", "Debidatta Dwibedi", "Michael Elabd", "Claudio Fantacci", "Cody Fong", "Erik Frey", "Chuyuan Fu", "Marissa Giustina", "Keerthana Gopalakrishnan", "Laura Graesser", "Leonard Hasenclever", "Nicolas Heess", "Brandon Hernaez", "Alexander Herzog", "R. Alex Hofer", "Jan Humplik", "Atil Iscen", "Mithun George Jacob", "Deepali Jain", "Ryan Julian", "Dmitry Kalashnikov", "M. Emre Karagozler", "Stefani Karp", "Chase Kew", "Jerad Kirkland", "Sean Kirmani", "Yuheng Kuang", "Thomas Lampe", "Antoine Laurens", "Isabel Leal", "Alex X. Lee", "Tsang-Wei Edward Lee", "Jacky Liang", "Yixin Lin", "Sharath Maddineni", "Anirudha Majumdar", "Assaf Hurwitz Michaely", "Robert Moreno", "Michael Neunert", "Francesco Nori", "Carolina Parada", "Emilio Parisotto", "Peter Pastor", "Acorn Pooley", "Kanishka Rao", "Krista Reymann", "Dorsa Sadigh", "Stefano Saliceti", "Pannag Sanketi", "Pierre Sermanet", "Dhruv Shah", "Mohit Sharma", "Kathryn Shea", "Charles Shu", "Vikas Sindhwani", "Sumeet Singh", "Radu Soricut", "Jost Tobias Springenberg", "Rachel Sterneck", "Razvan Surdulescu", "Jie Tan", "Jonathan Tompson", "Vincent Vanhoucke", "Jake Varley", "Grace Vesom", "Giulia Vezzani", "Oriol Vinyals", "Ayzaan Wahid", "Stefan Welker", "Paul Wohlhart", "Fei Xia", "Ted Xiao", "Annie Xie", "Jinyu Xie", "Peng Xu", "Sichun Xu", "Ying Xu", "Zhuo Xu", "Yuxiang Yang", "Rui Yao", "Sergey Yaroshenko", "Wenhao Yu", "Wentao Yuan", "Jingwei Zhang", "Tingnan Zhang", "Allan Zhou", "Yuxiang Zhou"], "published": "2025-03-25T19:02:56+00:00", "updated": "2025-03-25T19:02:56+00:00", "summary": "Recent advancements in large multimodal models have led to the emergence of remarkable generalist capabilities in digital domains, yet their translation to physical agents such as robots remains a significant challenge. This report introduces a new family of AI models purposefully designed for robotics and built upon the foundation of Gemini 2.0. We present Gemini Robotics, an advanced Vision-Language-Action (VLA) generalist model capable of directly controlling robots. Gemini Robotics executes smooth and reactive movements to tackle a wide range of complex manipulation tasks while also being robust to variations in object types and positions, handling unseen environments as well as following diverse, open vocabulary instructions. We show that with additional fine-tuning, Gemini Robotics can be specialized to new capabilities including solving long-horizon, highly dexterous tasks, learning new short-horizon tasks from as few as 100 demonstrations and adapting to completely novel robot embodiments. This is made possible because Gemini Robotics builds on top of the Gemini Robotics-ER model, the second model we introduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends Gemini's multimodal reasoning capabilities into the physical world, with enhanced spatial and temporal understanding. This enables capabilities relevant to robotics including object detection, pointing, trajectory and grasp prediction, as well as multi-view correspondence and 3D bounding box predictions. We show how this novel combination can support a variety of robotics applications. We also discuss and address important safety considerations related to this new class of robotics foundation models. The Gemini Robotics family marks a substantial step towards developing general-purpose robots that realizes AI's potential in the physical world.", "primary_category": "cs.RO", "categories": ["cs.RO"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2503.20020v1", "entry_id": "http://arxiv.org/abs/2503.20020v1", "cited_by_count": 3}
{"arxiv_id": "2503.14734v2", "title": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots", "authors": ["NVIDIA", ":", "Johan Bjorck", "Fernando Castañeda", "Nikita Cherniadev", "Xingye Da", "Runyu Ding", "Linxi \"Jim\" Fan", "Yu Fang", "Dieter Fox", "Fengyuan Hu", "Spencer Huang", "Joel Jang", "Zhenyu Jiang", "Jan Kautz", "Kaushil Kundalia", "Lawrence Lao", "Zhiqi Li", "Zongyu Lin", "Kevin Lin", "Guilin Liu", "Edith Llontop", "Loic Magne", "Ajay Mandlekar", "Avnish Narayan", "Soroush Nasiriany", "Scott Reed", "You Liang Tan", "Guanzhi Wang", "Zu Wang", "Jing Wang", "Qi Wang", "Jiannan Xiang", "Yuqi Xie", "Yinzhen Xu", "Zhenjia Xu", "Seonghyeon Ye", "Zhiding Yu", "Ao Zhang", "Hao Zhang", "Yizhou Zhao", "Ruijie Zheng", "Yuke Zhu"], "published": "2025-03-18T21:06:21+00:00", "updated": "2025-03-27T02:52:43+00:00", "summary": "General-purpose robots need a versatile body and an intelligent mind. Recent advancements in humanoid robots have shown great promise as a hardware platform for building generalist autonomy in the human world. A robot foundation model, trained on massive and diverse data sources, is essential for enabling the robots to reason about novel situations, robustly handle real-world variability, and rapidly learn new tasks. To this end, we introduce GR00T N1, an open foundation model for humanoid robots. GR00T N1 is a Vision-Language-Action (VLA) model with a dual-system architecture. The vision-language module (System 2) interprets the environment through vision and language instructions. The subsequent diffusion transformer module (System 1) generates fluid motor actions in real time. Both modules are tightly coupled and jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture of real-robot trajectories, human videos, and synthetically generated datasets. We show that our generalist robot model GR00T N1 outperforms the state-of-the-art imitation learning baselines on standard simulation benchmarks across multiple robot embodiments. Furthermore, we deploy our model on the Fourier GR-1 humanoid robot for language-conditioned bimanual manipulation tasks, achieving strong performance with high data efficiency.", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.LG"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2503.14734v2", "entry_id": "http://arxiv.org/abs/2503.14734v2", "cited_by_count": 3}
{"arxiv_id": "2506.07530v1", "title": "BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation", "authors": ["Hongyu Wang", "Chuyan Xiong", "Ruiping Wang", "Xilin Chen"], "published": "2025-06-09T08:15:11+00:00", "updated": "2025-06-09T08:15:11+00:00", "summary": "Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA.", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.CV"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2506.07530v1", "entry_id": "http://arxiv.org/abs/2506.07530v1", "cited_by_count": 0}
{"arxiv_id": "2503.02310v1", "title": "Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding", "authors": ["Wenxuan Song", "Jiayi Chen", "Pengxiang Ding", "Han Zhao", "Wei Zhao", "Zhide Zhong", "Zongyuan Ge", "Jun Ma", "Haoang Li"], "published": "2025-03-04T06:12:08+00:00", "updated": "2025-03-04T06:12:08+00:00", "summary": "Vision-Language-Action (VLA) models demonstrate remarkable potential for generalizable robotic manipulation. The performance of VLA models can be improved by integrating with action chunking, a critical technique for effective control. However, action chunking linearly scales up action dimensions in VLA models with increased chunking sizes. This reduces the inference efficiency. To tackle this problem, we propose PD-VLA, the first parallel decoding framework for VLA models integrated with action chunking. Our framework reformulates autoregressive decoding as a nonlinear system solved by parallel fixed-point iterations. This approach preserves model performance with mathematical guarantees while significantly improving decoding speed. In addition, it enables training-free acceleration without architectural changes, as well as seamless synergy with existing acceleration techniques. Extensive simulations validate that our PD-VLA maintains competitive success rates while achieving 2.52 times execution frequency on manipulators (with 7 degrees of freedom) compared with the fundamental VLA model. Furthermore, we experimentally identify the most effective settings for acceleration. Finally, real-world experiments validate its high applicability across different tasks.", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.CV"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2503.02310v1", "entry_id": "http://arxiv.org/abs/2503.02310v1", "cited_by_count": 0}
{"arxiv_id": "2506.07339v2", "title": "Real-Time Execution of Action Chunking Flow Policies", "authors": ["Kevin Black", "Manuel Y. Galliker", "Sergey Levine"], "published": "2025-06-09T01:01:59+00:00", "updated": "2025-12-05T07:35:35+00:00", "summary": "Modern AI systems, especially those interacting with the physical world, increasingly require real-time performance. However, the high latency of state-of-the-art generalist models, including recent vision-language action models (VLAs), poses a significant challenge. While action chunking has enabled temporal consistency in high-frequency control tasks, it does not fully address the latency problem, leading to pauses or out-of-distribution jerky movements at chunk boundaries. This paper presents a novel inference-time algorithm that enables smooth asynchronous execution of action chunking policies. Our method, real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out of the box with no re-training. It generates the next action chunk while executing the current one, \"freezing\" actions guaranteed to execute and \"inpainting\" the rest. To test RTC, we introduce a new benchmark of 12 highly dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging real-world bimanual manipulation tasks. Results demonstrate that RTC is fast, performant, and uniquely robust to inference delay, significantly improving task throughput and enabling high success rates in precise tasks $\\unicode{x2013}$ such as lighting a match $\\unicode{x2013}$ even in the presence of significant latency. See https://pi.website/research/real_time_chunking for videos.", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.LG"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2506.07339v2", "entry_id": "http://arxiv.org/abs/2506.07339v2", "cited_by_count": 0}
